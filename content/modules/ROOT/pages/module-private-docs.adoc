= Private Docs (RAG/Tools) - 45 minutes
:imagesdir: ../assets/images
:sectnums:

++++
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3HTRSDJ3M4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3HTRSDJ3M4');
</script>
++++

== Goals of this lab

Parasol Insurance is looking to not only build new apps but also *integrate generative AI into their existing applications*, such as their customer service representative tools used to respond to customer claim emails. The goal of this lab is to integrate private data from internal documents and other data sources into the existing application, improving accuracy of the responses from the assistant AI service and ultimately improving customer satisfaction. To achieve this goal, you will:

* Dive into advanced application integration techniques for AI, exploring how to *integrate* AI into applications for Parasol's unique needs
* Learn how to ingest and utilize *private* documents securely
* Apply the *Retrieval-Augmented Generation* (RAG) pattern to enhance AI model output
* Extend AI models with specialized *tools and agents*
* Understand the *limitations* of RAG and when to consider fine-tuning

This module will focus on *Building and Refining*.

image::private-docs/building-gen-ai.png[Adopting Gen AI]

IMPORTANT: If you haven't accessed *Red Hat Developer Hub and Red Hat Dev Spaces* yet, complete the following sections. Otherwise, *proceed* to the <<skip-prereqs, Working in your Cloud-based Development Environment>> section.

include::partial-devhub-pre-req.adoc[]
include::partial-cicd-pre-req.adoc[]
include::partial-dev-spaces-pre-req.adoc[]

[#skip-prereqs]
== Working in your Cloud-based Development Environment

https://developers.redhat.com/products/openshift-dev-spaces/overview[Red Hat OpenShift Dev Spaces^] is a cloud-based development environment built on top of Kubernetes and containers. It offers a streamlined and secure way for developers to code, build, and test applications directly within the OpenShift ecosystem. You'll use the Dev Spaces environment in this module to enhance the current functionality of the Parasol Insurance application.

=== Using LangChain4j with Quarkus

You'll be building new features in this application, based on https://developers.redhat.com/products/quarkus/overview[Quarkus^] and the https://github.com/langchain4j/langchain4j[LangChain4j^] library. The https://quarkus.io/extensions/?search-regex=langchain[Quarkus LangChain4j extensions^] bridge the gap between your Quarkus application and LangChain4j, a library that allows interaction with various LLMs like OpenAI, Hugging Face, or Ollama. It has the following key features and benefits:

** *Simplified LLM Integration*: The extension streamlines the process of incorporating LLMs into your application, saving development time and effort.
** *Declarative AI Services*: Defines how you want to interact with the LLM using annotations or configuration files.
** *Embedding Support*: Integrates with document embedding stores like Redis, Chroma, or Infinispan to store and retrieve document context for the LLM.
** *Observability Integration*: Allows monitoring and logging of LLM interactions within your Quarkus application.

include::partial-revert-devspaces-workspace.adoc[]

=== Understanding the application's codebase

This Quarkus application is a customer service processing tool that handles customer claim emails for Parasol insurance. The team has recently improved the application with a chatbot to interact with the agent and to generate responses based on the email content. In the VS Code environment, navigate to the `src/main/java/org/parasol` directory, which contains the main source code of the application.

image::private-docs/quarkus-codebase.png[Quarkus codebase]

In the `src/main/java/org/parasol/ai/ClaimService.java` file, you'll find the main AI chatbot class that processes the customer claim emails with a `@SystemMessage` and `@UserMessage` annotation, and a `chat` method. The `chat` method processes the claim details and question, then generates a response based on the claim and references provided.

TIP: If you're unfamiliar with what a system or user message is, you might want to read through the xref:module-prompt.adoc[Prompting Basics,window=_blank] module first.

.src/main/java/org/parasol/ai/ClaimService.java
[source,java,subs="+attributes,macros+"]
----
include::https://raw.githubusercontent.com/rh-rad-ai-roadshow/ai-llm-template/main/scaffolder-templates/parasol-java-template/skeleton/src/main/java/org/parasol/ai/ClaimService.java[]
----

You can also explore the `src/main/resources/application.properties` file to review the application configuration, including the Quarkus configuration and the LangChain4j extension settings.

.src/main/resources/application.properties
[source,properties,subs="+attributes,macros+"]
----
include::https://raw.githubusercontent.com/rh-rad-ai-roadshow/ai-llm-template/main/scaffolder-templates/parasol-java-template/skeleton/src/main/resources/application.properties[]
----

NOTE: The application is currently configured to be able to communicate with either an https://platform.openai.com/docs/api-reference/introduction[OpenAI-compliant^] or an https://github.com/ollama/ollama/blob/main/docs/api.md[Ollama-compliant^] endpoint. More on that later!

== Testing out the existing chatbot

Now that we've checked out the chatbot code and gotten to a clean state, let's test it locally.

In the terminal window that you opened earlier at the bottom of the editor, run the following command:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
./mvnw clean quarkus:dev -Dquarkus.otel.enabled=true -Dquarkus.otel.exporter.otlp.traces.endpoint=http://jaeger.parasol-app-{user}-dev.svc.cluster.local:4317
----

This will download some dependencies and start the application in Quarkus Dev Mode.

NOTE: You may get asked to contribute anonymous build time data. It's up to you what to answer, but we recommend answering yes so the Quarkus developers can have valuable performance insights to improve Quarkus even further.

After a minute or 2, depending on network speeds, your application will be up and running:

image::private-docs/quarkus-dev.png[Quarkus dev]

Go ahead and click the **Open In New Tab** button that shows up in the bottom right

[NOTE]
====
If you are prompted about allowing VSCode to open an external website, click **Open**.

image::private-docs/open-external-website.png[Open external website]
====

You should now see the Parasol UI in the new tab.

image::private-docs/parasol-ui.png[Parasol UI]

=== Click on the first claim with number **CLM195501** to open the claim

Then on the chat icon in the bottom right corner to open the chat interface.

image::devhub/claim_view.png[Chat interface]

=== In the chat interface, try asking the following questions about a specific claim

[.console-input]
[source,text,subs="+attributes,macros+"]
----
What is the status of this claim?
----

[.console-input]
[source,text,subs="+attributes,macros+"]
----
When was the incident reported?
----

[.console-input]
[source,text,subs="+attributes,macros+"]
----
What is the claimed amount?
----

[.console-input]
[source,text,subs="+attributes,macros+"]
----
What is the inception date of the policy?
----

image::devhub/chatbot_query.png[Chatbot questions]

You should notice that the chatbot can answer these questions based on the claim context we provided. Now, let's ask a more complex question that requires knowledge of Parasol's policies:

=== Ask the chatbot: "According to the policy term limit, should this claim be approved?"

image::private-docs/chat-policies-unknown.png[Chatbot questions about policies]

The answer you will get back might be different from the one in the screenshot above, but you'll likely observe that the chatbot struggles with this question, as it doesn't have access to Parasol's specific policies, *in particular the 6 month policy term limit*. It might even respond with an incorrect decision! If only there was a way to automatically retrieve this information and provide it to the chatbot.

== Embedding Documents with RAG

https://research.ibm.com/blog/retrieval-augmented-generation-RAG[Retrieval-Augmented Generation (RAG)^] is a technique that enhances language models by providing them with relevant information retrieved from a knowledge base. This is particularly useful when dealing with domain-specific knowledge or private data.

=== When to use RAG vs fine-tuning
Use Retrieval-Augmented Generation (RAG) when you need to access a dynamic knowledge base in real-time, especially for tasks that involve varied or constantly updating information. RAG is ideal if you require scalability, need to handle out-of-domain queries, or want to deploy quickly without the resource demands of fine-tuning - which often requires specialized knowledge of working with AI models, something application developers often don't possess.

On the other hand, choose fine-tuning when your task is specialized and you need precise control over the model's behavior. Fine-tuning is better suited for homogeneous data, offline or low-latency applications, and situations where security or compliance requires all data to be embedded within the model. It's also preferable when you have a well-defined use case with specific task requirements.

TIP: See the xref:module-ilab.adoc[AI Model Fine-Tuning module,window=_blank] for more information on fine-tuning models.

=== Add Parasol-specific policies to the LLM's knowledge base with RAG

While we could use a https://www.ibm.com/topics/vector-database[vector database^] to store vectorized data to use with RAG, we can also simply specify a directory where text-based files are stored. We can then parse this data, retrieve similar data, vectorize it, and pass it along to our LLM calls. This process is made very easy with the aptly named https://docs.quarkiverse.io/quarkus-langchain4j/dev/easy-rag.html["Easy RAG" extension in Quarkus^].

Return to Dev Spaces and open a new terminal window as you did before (click on the **hamburger** menu button in the top left, then click on **Terminal** and finally on **New Terminal**), or click the **+** button on the top-right of your current terminal.

image::private-docs/open-new-terminal.png[]

In the new terminal, add this extension by running the following command:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
./mvnw quarkus:add-extension -Dextension=easy-rag
----

You will see that a new extension was added to your application:

[.console-output]
[source,java]
----
[INFO] Looking for the newly published extensions in registry.quarkus.io
[INFO] [SUCCESS] âœ…  Extension io.quarkiverse.langchain4j:quarkus-langchain4j-easy-rag:xx.xx.xx has been installed
----

[NOTE]
====
Dev Spaces may also ask if you would like to open the application again. This is because the Quarkus dev mode in your other terminal has restarted due to adding a new extension.

Don't open the application again yet, we first need to add some configuration for the new extension.
====

[CAUTION]
====
You will most likely see errors if you open the application again (or return to the console the application is running in) before completing the following configuration. Please continue with the following configuration before reattempting to use the application.
====

We have already added a policy document (`policy-info.pdf`) in the `src/main/resources/policies` folder. https://gitlab-gitlab.{openshift_cluster_ingress_domain}/development/parasol-app-{user}/-/blob/master/src/main/resources/policies/policy-info.pdf[Click here^] to view it.

We can tell the Easy Rag extension where to find it by adding the following lines in your `src/main/resources/application.properties`:

[.console-input]
[source,properties,subs="+attributes,macros+"]
----
# RAG
quarkus.langchain4j.embedding-model.provider=openai<1>
quarkus.langchain4j.easy-rag.path=src/main/resources/policies<2>
quarkus.langchain4j.easy-rag.reuse-embeddings.enabled=true<3>
quarkus.langchain4j.openai.base-url=http://parasol-embedding-predictor.aiworkshop.svc.cluster.local:8080/v1<4>
quarkus.langchain4j.openai.embedding-model.model-name=parasol-embedding<5>
----
<1> Instructs Quarkus to use an OpenAI-compliant endpoint for computing embeddings
<2> Path to the folder that contains the RAG documents
<3> Reuse embeddings so when we restart the application it does not need to re-vectorize the documents
<4> The URL to the embedding model server being to compute and search embeddings
<5> The name of the embedding model being used to compute and search embeddings

TIP: In production, these embeddings would most likely be pre-computed as part of an outer-loop pipeline and stored into a vector database that the application would read from.

That's all there is to it! When Quarkus restarts it will read the `src/main/resources/policies/policy-info.pdf` file and compute vectorized embeddings for all the text in the document. It will then store these embeddings into an in-memory vector store. When chatting with the chatbot the application will first query the vector store for relevant information, which it will then pass to the LLM with the user's query.

Reload the browser window to make sure Quarkus Dev mode picks up the changes.

Now ask the same question in the chat window:

[.console-input]
[source,text,subs="+attributes,macros+"]
----
According to the policy term limit, should this claim be approved?
----

This time the bot should be able to answer the questions with accurate information based on the policies document!

image::private-docs/chat-response-rag.png[Chat response with RAG]

NOTE: The actual result you see may differ from the screenshot due to the non-deterministic nature of LLMs.

== Enhancing functionality with tools and agents

Until now we have had fairly straightforward interactions with the AI LLM model, in that we have asked it questions and it responded with some text and it was up to us to interpret the result.

Wouldn't it be nice if we could instruct the model to actually call some code in our application and "do" something? That's what the https://docs.quarkiverse.io/quarkus-langchain4j/dev/agent-and-tools.html["Tool"] concept is all about.

In this section we're going to instruct the LLM to call a piece of code that will change the status of a claim and send a notification email to the claimant.

For sending emails we will use the Quarkus https://quarkus.io/guides/mailer[Mailer^] and https://quarkus.io/extensions/io.quarkiverse.mailpit/quarkus-mailpit/?tab=docs[Mailpit^] extensions.

=== Add and configure mail extensions

In Dev Spaces, switch to the second terminal you opened previously (NOT the one currently running Quarkus dev mode).

Add the mailer and mailpit extensions to the Quarkus application by issuing the following command in this terminal.

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
./mvnw quarkus:add-extension -Dextensions="mailpit,mailer"
----

Open `src/main/resources/application.properties` and add the mail configuration:

[.console-input]
[source,properties]
----
# Mail
%dev.quarkus.mailer.mock=false
quarkus.mailer.tls=false
quarkus.mailpit.image-name=quay.io/rhappsvcs/mailpit:latest
----

=== Create Notification tool

Now let's create a new NotificationService class that will have the functionality to update claim status and send an email. Create a new file `NotificationService.java` in the `src/main/java/org/parasol/ai` folder.

image::private-docs/new-notification-service.png[Create New File]

image::private-docs/new-notification-service-file.png[Name file `NotificationService.java`]

Overwrite all the content in the file with the following:

.src/main/java/org/parasol/ai/NotificationService.java
[.console-input]
[source,java]
----
include::https://raw.githubusercontent.com/rh-rad-ai-roadshow/parasol-insurance/private-docs-rag-solution/app/src/main/java/org/parasol/ai/NotificationService.java[]
----

There is a fair bit of code in this file, but the main thing you should pay attention to is the `@Tool` annotation with the instruction "update claim status". This piece of natural language text you add to the annotation is interpreted by the LLM, which will now know that if you tell it to "update the claim status of my case", it should call the `updateClaimStatus` method with given parameters.

=== Wire tool to the chat service

To wire everything up, we will need to tell the AI Service about this tool. In the `src/main/java/org/parasol/ai` folder, open the `ClaimService.java` class again.

Find the `@RegisterAiService` annotation. We need to register the `NotificationService` class as a tool. Go ahead and replace the existing line with the following:

[.console-input]
[source,java,subs="+attributes,macros+"]
----
@RegisterAiService(modelName = "parasol-chat", tools = NotificationService.class)
----

By doing this we have now registered the `NotificationService` file as class that contains one or more `@Tool` annotated methods.

=== Convert chat from streaming to synchronous

We now need to make a few more changes. Currently when we chat with the assistant the responses stream back to us one word at a time. Unfortunately when using tools you can't use this feature. Instead, we need to make a request and wait for the entire response from the LLM.

Still in `src/main/java/org/parasol/ai/ClaimService.java`, replace the line

[source,java]
----
Multi<String> chat(ClaimBotQuery query);
----

with

[.console-input]
[source,java]
----
String chat(ClaimBotQuery query);
----

Then open `src/main/java/org/parasol/resources/ClaimWebsocketChatBot.java` and replace

[source,java]
----
@OnTextMessage
@WithSpan("ChatMessage")
public Multi<ClaimBotQueryResponse> onMessage(ClaimBotQuery query) {
    Log.infof("Got chat query: %s", query);

    return bot.chat(query)
      .invoke(response -> Log.debugf("Got chat response: %s", response))
      .map(resp -> new ClaimBotQueryResponse("token", resp, ""));
}
----

with

[.console-input]
[source,java]
----
@OnTextMessage
@WithSpan("ChatMessage")
public ClaimBotQueryResponse onMessage(ClaimBotQuery query) {
    Log.infof("Got chat query: %s", query);
    var response = new ClaimBotQueryResponse("token", this.bot.chat(query), "");
    Log.debugf("Got chat response: %s", response);

    return response;
}
----

IMPORTANT: When the application restarts you may see compilation errors in the console. This is because we updated the `ClaimWebsocketChatBot` class but did not update the corresponding tests. That is ok. The application will continue to function properly.

=== Configure new LLM with tool support

The last thing we need to do is to switch to a different LLM for our chat bot because the `parasol-chat` model does not currently support tools integration.

Open `src/main/resources/application.properties` and overwrite these lines:

[source,properties]
----
quarkus.langchain4j.openai.parasol-chat.chat-model.stop=DONE,done,stop,STOP
quarkus.langchain4j.openai.parasol-chat.chat-model.model-name=parasol-chat
quarkus.langchain4j.openai.parasol-chat.base-url=http://parasol-chat-predictor.aiworkshop.svc.cluster.local:8080/v1
----

with:

[.console-input]
[source,properties,subs="+attributes,macros+"]
----
# quarkus.langchain4j.openai.parasol-chat.chat-model.stop=DONE,done,stop,STOP
quarkus.langchain4j.openai.parasol-chat.chat-model.model-name=llama3.1
quarkus.langchain4j.openai.parasol-chat.base-url=http://parasol-tools.aiworkshop.svc.cluster.local/v1
----

Let's try it out! Go back to the browser and refresh the claim page for claim 195501 (Marty McFly).

Open the chat interface again and this time tell the chat assistant:

[.console-input]
[source,text,subs="+attributes,macros+"]
----
Update the claim status to denied
----

After a few moments (it could take up to a minute or so) you should see that the chatbot answers, telling you that it updated the status and sent an email to the customer.

image::private-docs/chat-response-email.png[Chatbot response]

If you refresh the chat page you should also see that the claim's status has now been changed to denied:

image::private-docs/claim-denied.png[Claim denied]

Since we're running this exercise in Quarkus Dev Mode, we can actually test if the email functionality actually works, thanks to the MailPit extension which provides a Quarkus Dev Service with a mock email server.

Go to the Quarkus Dev UI by opening the **Endpoints** view within Dev Spaces and finding **quarkus-devui**, then click the **Open in a new tab** button.

image::private-docs/open-quarkus-devui.png[Open Quarkus Dev UI]

image::private-docs/quarkus-dev-ui.png[Quarkus Dev UI]

Find the "Mailpit" section and click on the https://{user}-parasol-insurance-quarkus-devui.{openshift_cluster_ingress_domain}/q/mailpit/[link^] next to "Mailpit UI" and you will see a new email that was sent thanks to our newly added functionality!

NOTE: The mailpit section might not be exactly in the same location as in the screenshot above.

image::private-docs/mailpit.png[Mailpit UI]

Click on the email to open it

image::private-docs/mailpit-message.png[Mailpit message]

You can now verify that the user received the email!

== Observability of AI-infused applications

=== What is OpenTelemetry?

OpenTelemetry is a collection of tools, APIs, and SDKs. Use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) to help you analyze your software's performance and behavior.

OpenTelemetry was created by merging the popular https://opentracing.io/[OpenTracing^] and https://opencensus.io/[OpenCensus^] projects. It is a standard that integrates with many open source and commercial products written in many programming languages. Implementations of OpenTelemetry are in varying stages of maturity.

At its core, OpenTelemetry contains the https://opentelemetry.io/docs/collector[Collector^], a vendor-agnostic way to receive, process, and export telemetry data.

image::private-docs/otel-collector-architecture.png[OTel Collector Architecture]

=== Integrating OpenTelemetry into our application

Integrating OpenTelemetry is easy! In fact, simply adding the https://quarkus.io/guides/opentelemetry[`quarkus-opentelemetry`] extension to an application will automatically configure many other extensions to use it!

You might not have realized that everything you've done in this module thus far has been recorded and stored in a local https://opentelemetry.io/docs/collector/[OpenTelemetry Collector^] instance and can be observed in a local https://www.jaegertracing.io/[Jaeger^] instance, an open source distributed tracing platform.

Observability is automatically built into AI services created via `@RegisterAiService`. The traces follow through any tool invocations, and even down to the database queries used during the workflow.

Let's take a look!

=== Looking at our traces

. https://jaeger-parasol-app-{user}-dev.{openshift_cluster_ingress_domain}[Click here^] to open the Jaeger UI.
    - This Jaeger instance is specific to your instance of the application and does not contain any data from any other users in the workshop.
. Once in the Jaeger UI, expand the **Service** dropdown and choose the **insurance-app** service.
    - This narrows the scope of observed traces just to our application
. Expand the **Operation** dropdown and choose the **ChatMessage** operation.
    - This narrows the scope of observed traces just to the chat interactions with our chat bot.
    - Feel free to explore other traces afterwards!
. Click the **Find Traces** button.

You should see something resembling the following:

image::private-docs/find-jaeger-traces.png[Jaeger traces]

NOTE: Your screenshot may look different than what is shown above. You may have more or less traces.

Click on the top trace in the list to see a detailed view of that trace:

image::private-docs/jaeger-trace.png[Jaeger trace]

A trace consists of a series of _spans_. Each span is a time interval representing a unit of work. Spans often have parent/child relationships and form a hierarchy. Spans can also indicate the parallelization of work running concurrently. Each span also indicates its duration, so you can see exactly how much time each unit of work takes.

In our case there isn't any parallelization, since all the units of work execute sequentially. You can start to drill-down through the units of work to see what actually happened when you told the chat bot to update the claim's status to denied:

image::private-docs/jaeger-trace-details.png[Jaeger trace details]

. The entrypoint to the chat interface (`org.parasol.resources.ClaimWebsocketChatBot` class, `onMessage` method).
. Entrypoint into the `ClaimService` AI service's `chat` method.
. REST HTTP call to the embedding model to retrieve appropriate embeddings based on the chat query.
. Call to the `llama3.1` model.
. REST HTTP call to the model server.
. Callback from the model to invoke the `updateClaimStatus` tool.
. Call into our `NotificationService.updateClaimStatus` method.
. Get a connection to the database.
. **Select** the `Claim` record from the database to update.
. **Update** the `Claim` record from the database and set the claim's status to **denied**.
. Send the result of the tool invocation back to the `llama3.1` model.
. REST HTTP call to the model server including the result from the tool invocation.

You can click on any span to show more contextual information about the span:

image::private-docs/jaeger-trace-attributes.png[Trace attributes]

As you can see in the above screenshot, the arguments to the `NotificationService.updateClaimStatus` are shown so we know that the claim with `claimId=1` had it's status updated to `denied`.

For spans that encompass database interactions you can also see relevant information:

image::private-docs/jaeger-trace-database.png[Trace Database info]

For interactions with the AI model you can also see information related to the model, such as what model was requested, how many tokens were in the prompt, what the temperature was, as well as what the URL to the model was:

image::private-docs/jaeger-trace-model-http-info.png[Trace Model info]

This is all very useful information!

== Conclusion

We hope you have enjoyed this module!

You can now return to the terminal where Quarkus dev mode is running and terminate the application by using the `q` key or hitting `CTRL-C`.

Here is a quick summary of what we have learned:

- How to load source code into Dev Spaces
- How to run the chatbot in local development mode
- What Retrieval-Augmented Generation (RAG) is and how easy it is to use with Quarkus
- How to enhance a chat bot's functionality with your own business logic
- How to observe and debug the flow of an AI-infused application
